import time
from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize, word_tokenize

# Create a long input string that could potentially cause a ReDoS attack
long_input = "word " * 10000  # Repeat 'word' 10,000 times

# Define a function to test the execution time of tokenization functions
def test_tokenization_function(func, input_data):
    start_time = time.time()
    func(input_data)
    end_time = time.time()
    execution_time = end_time - start_time
    # If execution time is less than 2 seconds, consider it as passed
    return execution_time < 2

# Test each vulnerable function
results = []
results.append(test_tokenization_function(PunktSentenceTokenizer().tokenize, long_input))
results.append(test_tokenization_function(sent_tokenize, long_input))
results.append(test_tokenization_function(word_tokenize, long_input))

# Check if all tests passed
if all(results):
    print('The ACI unit test is passed!!!')
else:
    print('The ACI unit test is failed!!!')
